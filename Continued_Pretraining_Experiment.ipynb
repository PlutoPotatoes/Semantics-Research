{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dbPSAU1mvEE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import pipeline, DataCollatorForLanguageModeling, Trainer, TrainingArguments, BertTokenizer, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: continued pretraining with BERT"
      ],
      "metadata": {
        "id": "24wJUmg1nYtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEMP BLOCK FOR TESTING DATA SET + TOKENIZER\n",
        "ds = load_dataset(\"ErikCikalleshi/new_york_times_news_1987_1995\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0YexbLbssQoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEMP BLOCK FOR TESTING DATA SET + TOKENIZER\n",
        "print(ds[\"train\"][:10])\n",
        "unique_dates = list(set(sorted(ds['train']['date'])))\n",
        "print(unique_dates)\n",
        "custom_date_tokens = [f\"<year_{d}>\" for d in unique_dates]\n",
        "print(custom_date_tokens)"
      ],
      "metadata": {
        "id": "beBDCeAbucqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer + model init\n",
        "\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "custom_token = custom_date_tokens\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "iqnKEDE2oT2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer))\n",
        "# 1. Add the special token to the tokenizer\n",
        "for i in custom_token:\n",
        "\n",
        "  tokenizer.add_special_tokens({'additional_special_tokens': [i]})\n",
        "\n",
        "# 2. Resize the model embeddings to account for the new token\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "GgLE5DRZvmkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add date token to data text and tokenize\n",
        "data = load_dataset(\"ErikCikalleshi/new_york_times_news_1987_1995\", split='test[:1%]')\n",
        "for row in data:\n",
        " row['content'] = f'<year_{row['date']}>' + row['content']\n"
      ],
      "metadata": {
        "id": "BcsMEsbnImuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add date token to data text and tokenize\n",
        "#Should tokenize to token id's now\n",
        "\n",
        "i = 0\n",
        "def add_date_tokens(examples):\n",
        "    examples['content'] = f'<year_{examples['date']}> ' + examples['content']\n",
        "    return examples\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  result = tokenizer(examples[\"content\"])\n",
        "  if tokenizer.is_fast:\n",
        "      result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "# Apply to dataset - ensure you don't remove the 'date' column until after mapping\n",
        "data = data.map(add_date_tokens)\n",
        "tokenized_dataset = data.map(tokenize_function, batched=True, remove_columns=data.column_names)\n",
        "print(tokenized_dataset.to_list())"
      ],
      "metadata": {
        "id": "-5upqPJLoZYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # `paragraphs` is a list of lists of lists\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next\n",
        "\n",
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i], paragraph[i + 1], paragraphs)\n",
        "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph"
      ],
      "metadata": {
        "id": "OE502RcFoFhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use these to set up the missing tokens to bert to predict\n",
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # For the input of a masked language model, make a new copy of tokens and\n",
        "    # replace some of them by '<mask>' or random tokens\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
        "    # language modeling task\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80% of the time: replace the word with the '<mask>' token\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10% of the time: keep the word unchanged\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position]\n",
        "            # 10% of the time: replace the word with a random word\n",
        "            else:\n",
        "                masked_token = random.choice(vocab.idx_to_token)\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels\n",
        "\n",
        "def _get_mlm_data_from_tokens(tokens, vocab):\n",
        "  candidate_pred_positions = []\n",
        "  # `tokens` is a list of strings\n",
        "  for i, token in enumerate(tokens):\n",
        "      # Special tokens are not predicted in the masked language modeling\n",
        "      # task\n",
        "      if token in ['<cls>', '<sep>']:\n",
        "          continue\n",
        "      candidate_pred_positions.append(i)\n",
        "  # 15% of random tokens are predicted in the masked language modeling task\n",
        "  num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "  mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "      tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
        "  pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                      key=lambda x: x[0])\n",
        "  pred_positions = [v[0] for v in pred_positions_and_labels]\n",
        "  mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
        "  return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iWwLYdkPZh-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html#generating-the-masked-language-modeling-task\n",
        "^ article on pretraining\n",
        "\n",
        "We need to follow the masked language prediction steps I believe"
      ],
      "metadata": {
        "id": "7neNjhYgSzee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class nyt_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, samples, max_len, tokenizer):\n",
        "    samples = self.prep_content(samples)\n",
        "    print(samples)\n",
        "    samples = self.tokenize_samples(tokenizer, samples)\n",
        "\n",
        "  def prep_content(self, examples):\n",
        "      sentences = []\n",
        "      for example in examples:\n",
        "        paragraph = example['content'].split('.')\n",
        "        paragraph = [f'<year_{example['date']}> {sentence}' for sentence in paragraph]\n",
        "        sentences.append([sentence for sentence in paragraph])\n",
        "      if '' in sentences:\n",
        "        sentences.remove('')\n",
        "      return sentences\n",
        "\n",
        "  def tokenize_samples(self, tokenizer, samples):\n",
        "    result = tokenizer(text=str(samples), padding='max_length')\n",
        "    print(result)\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "\n",
        "nyt_dataset(data, 512, tokenizer)"
      ],
      "metadata": {
        "id": "V8GKMs1WliWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pretraining functions\n",
        "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
        "                         segments_X, valid_lens_x,\n",
        "                         pred_positions_X, mlm_weights_X,\n",
        "                         mlm_Y, nsp_y):\n",
        "    # Forward pass\n",
        "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
        "                                  valid_lens_x.reshape(-1),\n",
        "                                  pred_positions_X)\n",
        "    # Compute masked language model loss\n",
        "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
        "    mlm_weights_X.reshape(-1, 1)\n",
        "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
        "    # Compute next sentence prediction loss\n",
        "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "    l = mlm_l + nsp_l\n",
        "    return mlm_l, nsp_l, l\n",
        "\n",
        "\n",
        "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
        "    net(*next(iter(train_iter))[:4])\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    step = 0\n",
        "\n",
        "    num_steps_reached = False\n",
        "    while step < num_steps and not num_steps_reached:\n",
        "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
        "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
        "            tokens_X = tokens_X.to(devices[0])\n",
        "            segments_X = segments_X.to(devices[0])\n",
        "            valid_lens_x = valid_lens_x.to(devices[0])\n",
        "            pred_positions_X = pred_positions_X.to(devices[0])\n",
        "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
        "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
        "            trainer.zero_grad()\n",
        "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
        "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
        "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "\n",
        "            step += 1\n",
        "            if step == num_steps:\n",
        "                num_steps_reached = True\n",
        "                break\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfY2MxmLeELx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pretraining\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./content/Training Data\",\n",
        "    num_train_epochs=3,\n",
        "    remove_unused_columns=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=500,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Start pretraining\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZAzTeCkwodVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}